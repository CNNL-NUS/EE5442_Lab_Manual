<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Pytorch &amp; Neural Networks</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(50, 48, 44, 1);
}
.highlight-gray_background {
	background: rgba(248, 248, 247, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(248, 243, 252, 1);
}
.highlight-pink_background {
	background: rgba(252, 241, 246, 1);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(248, 248, 247, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(248, 243, 252, 1);
}
.block-color-pink_background {
	background: rgba(252, 241, 246, 1);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: undefined; }
.select-value-color-pink { background-color: rgba(225, 136, 179, 0.27); }
.select-value-color-purple { background-color: rgba(168, 129, 197, 0.27); }
.select-value-color-green { background-color: rgba(123, 183, 129, 0.27); }
.select-value-color-gray { background-color: rgba(84, 72, 49, 0.15); }
.select-value-color-transparentGray { background-color: undefined; }
.select-value-color-translucentGray { background-color: undefined; }
.select-value-color-orange { background-color: rgba(224, 124, 57, 0.27); }
.select-value-color-brown { background-color: rgba(210, 162, 141, 0.35); }
.select-value-color-red { background-color: rgba(244, 171, 159, 0.4); }
.select-value-color-yellow { background-color: rgba(236, 191, 66, 0.39); }
.select-value-color-blue { background-color: rgba(93, 165, 206, 0.27); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="7d9a6975-f78c-4951-8109-8215f84d661a" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">4️⃣</span></div><h1 class="page-title">Pytorch &amp; Neural Networks</h1><p class="page-description"></p></header><div class="page-body"><p id="902f5182-1c3f-482e-99ac-bcf708919c27" class="">
</p><p id="5b56a555-2ff7-4078-8683-1e3a920ae060" class="">This Lab-1 will introduce some of the basic concepts of the Pytorch library. How it will be used to build small neural networks and train then with a well developed dataset. Also at the end run some inferencing on the trained model to test its performance. Some of the concepts here will be discussed and covered during the lectures.</p><p id="30ed8d36-62d4-46dc-bc74-4ac5c97f1c81" class="">
</p><p id="31336016-116d-40f9-8d7b-0f872d8aff44" class="">Some of the best references are provided in the <mark class="highlight-blue"><strong><a href="Basic%20Coding%20Framework%20b5c7294a2be74534b1b7549d94d11b80.html">Prerequisites page</a></strong></mark>, please refer to the videos to learn the fundamentals. The official page for pytorch can be access from here <mark class="highlight-blue"><strong><a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Pytorch</a></strong></mark>.</p><p id="a0eb5b4e-5d49-4e6e-8fb5-ad3fe12a3156" class="">
</p><p id="6a382980-bed3-4817-b404-0ca09e1ad450" class="">A small introduction will be given on the quantization aspects and how to implement FP16, for the others reference links shall be provided for the students to support them in performing the requirements of Milestone-1.</p><p id="17a3d192-8c64-8052-bbcb-e7b4ebe9c4f7" class="">
</p><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">Pytorch</summary><div class="indented"><p id="189c6514-cc09-4e34-ae78-50c9a5a952ba" class="">Pytorch is a widely used opensource library working under the Python framework. Developed initially by Meta AI and now spearheaded by Linux group. The main focus of this library is to give the users support to build machine learning models and applications. </p><p id="9aea7b09-ad93-4750-8ff7-564880addc58" class="">
</p><p id="e611359e-83dd-49c8-9788-e8106c21cc4d" class="">Pytorch allows the script to be run on a Graphics processor or GPU. Which can speed up the math computations. (<em>ChatGPT was also trained on such 10,000 GPUs and the inferencing is also performed on them</em>.) In cases where there is no Cuda driver available (Nvidia GPUs), pytorch switches over to the cpu, or can direct it to use ROCm from the AMD GPUs.</p><p id="8a101166-a3e6-4ff2-a17d-09f7f2b9c74a" class="">
</p><p id="6c102a18-04d8-46a2-b84a-191a8dd034e4" class="">The data, arrays or matrices can be sent to different devices using pytorch for computation. This can be achieved with the inbuilt framework of moving the tensor data to different processors.</p><p id="8b5dfa62-9c86-4dd6-9c46-57503e071e9c" class="">
</p><ol type="1" id="4f72c3a6-eaa9-4b8a-97c5-53a4815c7e49" class="numbered-list" start="1"><li>Import Pytorch then create and operate on basic tensors.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="ad4fd649-005d-4423-8717-3de85cf67494" class="code"><code class="language-Python"># Importing pytorch
import torch
import torch.nn as nn

# Generate an array of values between 0, 5 in step of 1
t1 = torch.linspace(0, 1, 5)
t2 = torch.arange(48).reshape(3,4,4)
print(&quot;t1&quot;, t1)
print(&quot;t2&quot;, t2)

# Broadcasting Rules, applying math operators on the tersor matrices
t3 = torch.ones((6,5))
t4 = torch.arange(5).reshape((1,5))
print(&quot;t3+t4\n&quot;, t3+t4)

# Other operation examples
t5 = torch.tensor([0.5, 1, 3, 4])
print(&quot;Mean&quot;, torch.mean(t5))
print(&quot;std&quot;, torch.std(t5))
print(&quot;max&quot;, torch.max(t5))
print(&quot;min&quot;, torch.min(t5))</code></pre></li></ol><p id="babf04e7-d8a3-47bb-8eb5-952a02dfd4ae" class="">
</p><ol type="1" id="aa43c475-2975-4dbe-b812-df5932645a9b" class="numbered-list" start="2"><li>There are many methods that are already predeveloped in this library, especially focusing on the requirements of image processing, machine learning, and neural networks.<p id="210f9e31-42bb-44d5-811b-28fda9b5eae0" class="">
</p><p id="37754074-5251-4cbf-b19a-b698104106d5" class="">Multiplying two matrices is very crucial in neural networks and another operation would be convolution. One such powerful method is computing of gradients. This is the core idea behind backpropagation and usage of loss functions.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="57535146-d600-49b7-a740-018ec251b8b7" class="code"><code class="language-Python"># Create a tensor matrix, and then add in the function (x^3)
x = torch.tensor([[5., 8.], [4., 6.]], requires_grad=True)
y = x.pow(3).sum()

# Now to compute the gradient or differentiate the function
comp_grad = y.backward()
print(&#x27;Gradient&#x27;, x.grad)

# Compare with the traditioal math result (3x^2)
diff_fx = 3*x**2
print(&#x27;Math result&#x27;, diff_fx)

# Performing matrix multplication
t1 = torch.randn((10, 10))
t2 = torch.randn((10, 10))
mat_mul = torch.matmul(t1, t2)
print(&#x27;Matrix Multiplication&#x27;, mat_mul)</code></pre></li></ol><p id="df53a1be-b8df-4d20-b93b-41ead6ae68a8" class="">
</p></div></details><p id="1793d192-8c64-8030-ba28-dc1fc0a7ff43" class="">
</p><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">Neural Networks</summary><div class="indented"><p id="ef4e6567-08fd-4c92-a3dd-62bebfddc246" class="">Computer systems and architectures that mimic the connections of biological neurons. The basic element is a neuron, here its known as the perceptron, which can store a value/weight and then add some bias to the value when transferring the data. This system doesn’t mimic all the feature sets of a human brain, but it has given a new dimension in computing and recognizing patterns in large amounts of data.</p><p id="c2758547-82e3-41d5-845d-1e1cd58cfb80" class="">
</p><p id="0146439d-4f74-4668-b209-74318a15e47f" class="">Some of the models and methods were proposed more than 3 decades back, but the processor power and computational capability was limited. Hence took a back seat. With the advent of FinFET transistors giving more density and the Graphics processing with multi-core DSP the neural network based algorithms sprang back to life in the past decade. </p><p id="557547bd-0dd5-40a8-b0af-62298b2b5d25" class="">
</p><p id="e82d78a3-8b5e-45e9-b6ee-9ccac4256df8" class="">In this era we have also witnessed other forms of these neural networks for various applications. Some of them like RNN (Recurrent Neural Network), LSTM (Long Short Term Memory), GAN (Generative Adversarial Network), and Transformers are cornerstones for the pace of growth in the AI world.</p><p id="65b7bde5-db48-4500-9b24-b38ff1528121" class="">
</p><figure id="a16cb54a-8dfd-4586-be25-24d9c80bd616" class="image" style="text-align:center"><a href="Untitled%205.png"><img style="width:624px" src="Untitled%205.png"/></a><figcaption>Input and weights representation <em>(Pablo Ruiz)</em></figcaption></figure><p id="e63d5263-e6f1-4660-9b17-e363f7483853" class="">
</p><figure id="1f7ab6bc-59ac-468e-a73e-d18e7457e5c0" class="image" style="text-align:center"><a href="Untitled%206.png"><img style="width:624px" src="Untitled%206.png"/></a><figcaption>Multiplying the weight matrix and adding the bias vector to the input matrix <em>(Pablo Ruiz)</em></figcaption></figure><p id="911819b9-9c27-480b-8d09-994f6cb10bd4" class="">
</p><p id="958df010-daea-4994-ac6a-6c4b616b9fc5" class="">Here a simplistic implementation with a few linear layers and an hidden layer will be performed. The activation functions along with the loss function and backpropagation will be explored (<em>Parts of this is derived from </em><em><mark class="highlight-blue"><strong><a href="https://www.youtube.com/playlist?list=PLkdGijFCNuVk9fO1IMfdV1Igob0FUHhkB">Luke Polson’s</a></strong></mark></em><em> work</em>). Further in the next section an hands on for CNN (Convolutional Neural Network) is provided. </p><p id="deecd609-4e70-4d5d-86e2-19886545ba3e" class="">Another good reference to learn to build neural network and understand their intricacies <mark class="highlight-blue"><strong><a href="https://www.youtube.com/playlist?list=PLPTV0NXA_ZSj6tNyn_UadmUeU3Q3oR-hu">Build Neural Networks from Scratch</a></strong></mark><mark class="highlight-default">. Developed by PhD graduates from MIT.</mark></p><p id="53d996f9-80d2-420b-b4d6-e12fbbbdc9a6" class="">
</p><ol type="1" id="0c0b91de-499e-4509-9b44-613c474f6713" class="numbered-list" start="1"><li>Create a data set for the inputs and validation for comparing the output. Consider a matrix 2x4 which as X and it maps to the outputs 1x4. This relationship between the input and the output will be learnt by the neural network. </li></ol><ol type="1" id="833c5955-8b1b-456a-ad9c-c07b25b73278" class="numbered-list" start="2"><li>Two linear layers are implemented here. Pytorch library has the built-in “nn” module which can be invoked to create the layers of the network. The weights of this network is decided randomly initially.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="03fcbbd0-2bd7-40d0-857e-241b6fb2e3c5" class="code"><code class="language-Python">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np                 # For matrix computation
import matplotlib.pyplot as plt    # For plotting

# Create the input and validation data
x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()
y = torch.tensor([1,5,2,5]).float()

# Neural network with two linear layers
class tryNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.mat1 = nn.Linear(2,8,bias=False)
        self.mat2 = nn.Linear(8,1,bias=False)
        
    # Here mat x is the input data
    def forward(self, x):
        x = self.mat1(x)
        x = self.mat2(x)
        return x.squeeze()
        
network_nn = tryNN()
y_computed = f(x)
print(&#x27;y_computed by NN:&#x27;, y_computed)
print(&#x27;Original validation output Y:&#x27;, y)</code></pre></li></ol><p id="87528af3-eac9-4455-b992-af3f359c32ff" class="">
</p><ol type="1" id="d1441f07-664c-4154-baca-1ebff4fd5757" class="numbered-list" start="3"><li>Now to adjust the computed Y to get closer to the original output required, a loss function is computed which here is a mean square error a statistical method to compare the closeness of the generated data to the required data.<p id="e11ad01c-a645-42c5-9765-e33080703e83" class="">
</p><p id="650c50d8-0cf5-46c2-be1b-ff24f027625b" class="">Once the loss function is computed the next step would be to adjust the loss based on Stochastic Gradient Descent, where the goal is to reduce the loss in the subsequent iterations (or Epochs). </p><p id="a18117ce-5e12-477d-b146-3d4cd6dbe09f" class="">
</p><p id="b9b3b312-f456-440b-810d-8c20509e8b90" class="">Here the learning rate will determine how fast the network learns, or how slow with better update. If the learning rate is too slow we’ll have a vanishing gradient where the next change or loss would be too small to distinguish with the chain derivative.</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="27857e33-c011-4f54-8c5f-88a6baed5ad1" class="code"><code class="language-Python"># Loss funcitons SSR
L = nn.MSELoss()
L(y, y_computed)

# Optimizer SGD
opt = torch.optim.SGD(f.parameters(), lr=0.001)

# Epochs
losses = []
for epochs in range(50):
    opt.zero_grad()
    loss_value = L(f(x), y)
    loss_value.backward()
    opt.step()
    losses.append(loss_value.item())

y_computed = f(x)
print(&#x27;y_computed by NN:&#x27;, y_computed)
print(&#x27;Original validation output Y:&#x27;, y)

# Plot the loss vs epoch
plt.plot(losses)
plt.xlabel(&#x27;Epoch&#x27;)
plt.ylabel(&#x27;Loss&#x27;)</code></pre></li></ol><ol type="1" id="e9c98346-4170-417a-8deb-3a8e3677b5d8" class="numbered-list" start="4"><li>The results are not that accurate, to improve the performance, additional activation function layer is used with an hidden layer. Here the method to save of the model is shown. Here the nodes in the linear layer is increased to 2x80 and 80x1. And adding an hidden layer of 80x80. The training is performed with 5000 epochs and 0.001 learning rate.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="43db11b0-ff32-41f9-bcbd-8f39e43330cd" class="code"><code class="language-Python">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np                
import matplotlib.pyplot as plt

# Device GPU or CPU
device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)

# Create the input and validation data
x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()
y = torch.tensor([1,5,2,5]).float()

class tryNN2(nn.Module):
    def __init__(self):
        super().__init__()
        self.mat1 = nn.Linear(2,80,bias=False)
        self.mat2 = nn.Linear(80,80)
        self.mat3 = nn.Linear(80,1,bias=False)
        self.relu = nn.ReLU()
        
    # Here mat x is the input data
    def forward(self, x):
        x = self.mat1(x)
        x = self.relu(x)
        x = self.mat2(x)
        x = self.relu(x)
        x = self.mat3(x)
        return x.squeeze()

# training
def train_model(x, y, f, n_epoch):
    opt = torch.optim.SGD(f.parameters(), lr=0.001)
    L = nn.MSELoss()
    
    losses = []
    for epochs in range(n_epoch):
        opt.zero_grad()
        loss_value = L(f(x), y)
        loss_value.backward()
        opt.step()
        losses.append(loss_value.item())
    return f, losses

# Data input and labels
x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float().to(device)
y = torch.tensor([1,5,2,5]).float().to(device)
f2 = tryNN2().to(device)

# Training
f2, losses = train_model(x, y, f2, n_epoch=5000)
y_computed = f2(x)
print(&#x27;y_computed by NN:&#x27;, y_computed)
print(&#x27;Original validation output Y:&#x27;, y)

# Saving the trained model
quant_model = tryNN2().to(device)
torch.save(quant_model.state_dict(), &#x27;linear_nn_relu.pt&#x27;)</code></pre></li></ol></div></details><p id="1793d192-8c64-80bb-bfb1-d8cdf9892fc0" class="">
</p><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">Convolutional Neural Networks</summary><div class="indented"><p id="d399d3cc-beac-4301-ad48-5a1c0f0c2d26" class="">Convolution in essence is defined as the integral of the product of the two functions after one is reflected about the y-axis and shifted. This is an operation performed on a matrix of values with another matrix called the kernel.</p><p id="65b7ceb7-8683-4e6e-ad99-77554e95399e" class="">
</p><p id="6974b46a-8c9a-41fc-a79c-e775a3e4cb9b" class="">One of the effective feature of performing convolution on image data is extracting the features. Manually creating filters to find edges and other feature sets to identify an object is an uphill challenge. Added to this for each object the parameters change, and also based on it spatial variations. This is where convolution operations with kernel matrices with multiple stage can bifurcate and learn the features in the image.</p><p id="b554ac56-3912-4a28-9629-1da789fcda6b" class="">
</p><figure id="63ce8c65-35e6-423b-b10d-4eb7ad7ee1ee" class="image" style="text-align:center"><a href="Untitled%207.png"><img style="width:576px" src="Untitled%207.png"/></a><figcaption>Convolution performed with the Kernel and resulting matrix - <em>(IBM)</em></figcaption></figure><p id="37e38d45-4021-4a8e-bf33-a8601ce15998" class="">
</p><p id="e8d6dc13-9e69-410d-b43d-634770714bc9" class="">Additional support is provided by the activation functions and fully connected layers in the subsequent steps, which help in classifying the image based on the features detected. Between convolution operations, pooling is performed to reduce the dimension of the data and retain as much as possible. This pooling is majorly of two types, averaging and maximum. Where a kernel of values are selected from the data and then the average or maximum value of the kernel is retained for further operations.</p><p id="d3ddd6a0-bc90-40eb-aefd-465f8a4276a3" class="">
</p><figure id="d46c427f-4fd1-4f76-b0df-88e06ef7bbbb" class="image" style="text-align:center"><a href="Untitled%208.png"><img style="width:384px" src="Untitled%208.png"/></a><figcaption>Pooling after convolution operation in LeNet-5</figcaption></figure><p id="6977681d-916a-461f-b0d4-fa7a6a066fcd" class="">
</p><p id="9e6ba727-473d-4363-a505-84ff7279b7d0" class="">References to learn are provided in the <mark class="highlight-blue"><strong><a href="Basic%20Coding%20Framework%20b5c7294a2be74534b1b7549d94d11b80.html">Prerequisites</a></strong></mark><mark class="highlight-default">, to learn more about CNN. An implementation of CNN is done in this lab session, with the popular LeNet-5 architecture having 5 layers comprising of the two 2D-convolution layers.</mark></p><p id="e1f31e2e-8438-4305-a465-64423e6a301d" class="">
</p><figure id="d168fa3e-d7e4-4dc3-ab52-f40f9d1a6013" class="image" style="text-align:center"><a href="Untitled%209.png"><img style="width:624px" src="Untitled%209.png"/></a><figcaption>LeNet-5 architecture - <em>(LeChunn, Fukushiwa, 1998)</em></figcaption></figure><p id="0eca5934-66ab-4c77-a6ac-d66ea2a62fa0" class="">
</p><p id="d2c5754f-4f8f-4d78-96fa-2dab60ec075d" class="">Training the LeNet-5 network on the standard MNIST handwritten dataset. The dataset should be downloaded from an online archive, similarly for other datasets as well <em>(Fashion MNIST, CIFAR10, &amp; CIFAR100)</em>. The other advantage is this dataset is cleaned and statistically reliable for the neural network training, in situations where the data is self created, additional steps to clean and label the data has to be performed. </p><p id="9a44af23-8943-4f7c-9c23-22c0cd70f16a" class="">
</p><p id="0fec0887-f306-4532-8c0e-f0de7d29a84f" class="">The dataset downloaded is divided into two sets one for training and another for validation. The MNIST data contains the image of handwritten digits along with the labels, in the real data one creates, labelling is an important without which training doesn’t yield results. Also there would be no benchmark to validate against.</p><p id="e033ca8c-5f2a-4afc-8b00-46f3365c7a08" class="">
</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="e4fa3fb7-e45d-41ac-a09c-d800bbee11fb" class="code"><code class="language-Python">import torch
import torchvision
import time
import os
import numpy as np

import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import matplotlib.pyplot as plt


device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)

# Control parameters
l_r = 0.001
n_epoch = 5
n_datasets = 64
n_classes = 10

# Load the data set for training and testing
# Can be loaded from the server or local

# Applying transform on the input data, rezise to 32x32, convert to tensor, normalize the data
transform_on_train_data = transforms.Compose([transforms.Resize((32,32)),
                                              transforms.ToTensor(),
                                              transforms.Normalize(mean=(0.1307,), std=(0.3081,))])
                                            
transform_on_test_data = transforms.Compose([transforms.Resize((32,32)),
                                            transforms.ToTensor(),
                                            transforms.Normalize(mean=(0.1325,), std=(0.3105,))])

# Loading the training data and test data to/from path, download will control the action
# For training data it is 1, for downloading from server it is 1
data_train = torchvision.datasets.MNIST(root=&#x27;C:/Users/vivek/Documents/GitHub/Python_Physics/stat_quest/mnist&#x27;,
                                        train=True,
                                        transform=transform_on_train_data,
                                        download=False)

data_test = torchvision.datasets.MNIST(root=&#x27;C:/Users/vivek/Documents/GitHub/Python_Physics/stat_quest/mnist&#x27;,
                                       train=False,
                                       transform=transform_on_test_data,
                                       download=False)

# Loading the training and test data according to requirement / parameters
# Load in the dataset size / batches required
# To shuffle the data sequence
load_data_train = torch.utils.data.DataLoader(dataset=data_train,
                                              batch_size=n_datasets,
                                              shuffle=True)
load_data_test = torch.utils.data.DataLoader(dataset=data_test,
                                             batch_size=n_datasets,
                                             shuffle=True)</code></pre><p id="9a40982c-23eb-4a92-a6e6-e1cd8f02d0ec" class="">
</p><p id="93cf069d-ef05-4967-a328-f4d62a7f7e87" class="">→ CNN with LeNet-5</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="bd0069b0-6146-4fe0-a707-01b06fe3d07d" class="code"><code class="language-Python">#Defining the convolutional neural network
class LeNet5_mnist(nn.Module):
    def __init__(self, num_classes):
        super(ConvNeuralNet, self).__init__()
        self.layer1 = nn.Sequential(
	            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),
		          nn.BatchNorm2d(6),
	            nn.ReLU(),
	            nn.MaxPool2d(kernel_size = 2, stride = 2))
        self.layer2 = nn.Sequential(
	            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),
	            nn.BatchNorm2d(16),
	            nn.ReLU(),
	            nn.MaxPool2d(kernel_size = 2, stride = 2))
        self.fc = nn.Linear(400, 120)
        self.relu = nn.ReLU()
        self.fc1 = nn.Linear(120, 84)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(84, num_classes)
        
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc(out)
        out = self.relu(out)
        out = self.fc1(out)
        out = self.relu1(out)
        out = self.fc2(out)
        return out
        
# Hyperparameter tuning
model = LeNet5(num_classes).to(device)
cost = nn.CrossEntropyLoss()           #Setting the loss function

#Setting the optimizer with the model parameters and learning rate
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

#this is defined to print how many steps are remaining when training
total_step = len(train_loader)</code></pre><p id="ad4c8f96-e1c2-4d3c-89db-647f021d37f0" class="">
</p><p id="88a2bb65-106d-4946-ace0-2eea44f62758" class="">→ Training and Validation</p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="f1994286-3012-4c80-8476-0bbd67f3db43" class="code"><code class="language-Python"># Traning and keep track of each epoch
total_step = len(load_data_train)
for epoch in range(n_epoch):
    for i, (images, labels) in enumerate(load_data_train):  
        images = images.to(device)
        labels = labels.to(device)
        
        #Forward pass
        outputs = model(images)
        loss = cost(outputs, labels)
        	
        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        		
        if (i+1) % 400 == 0:
            print (&#x27;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}&#x27; 
        		           .format(epoch+1, n_epoch, i+1, total_step, loss.item()))
        		           
# Saving the trained model
quant_model = LeNet5_mnist().to(device)
torch.save(quant_model.state_dict(), &#x27;CNN_mnist.pt&#x27;)

# Validate the model, check the accuracy of the trained model
# In test phase, we don&#x27;t need to compute gradients (for memory efficiency)
with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in load_data_test:
        images = images.to(device)
        labels = labels.to(device)
        
        # Input the images to the trained model and store the outputs, then validate
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    print(&#x27;Accuracy of the network on the 10000 test images: {} %&#x27;.format(100 * correct / total))
	 </code></pre></div></details><p id="1793d192-8c64-80d0-b919-e76d608f8d33" class="">
</p><details open=""><summary style="font-weight:600;font-size:1.875em;line-height:1.3;margin:0">Quantization</summary><div class="indented"><p id="d7dc8eed-22ba-4f8b-a1f5-7e3a34ebad68" class="">The datatype of the dataset and the model is default “float32”.  The GPUs on the server can support single precession (float32) and double precession (float64). To optimize the training for other hardware platforms the training will be performed with quantized data and model. </p><p id="c1ca5ae5-38b9-49eb-848f-6713d0a58c22" class="">
</p><p id="f1943008-b828-43ac-8e72-2b2783da1ba7" class="">Based on the requirement and the hardware accessibility the quantization can be performed. There are various in-built methods in Pytorch to perform this post training, or during the training process, and complete quantization of the dataset and the model being trained. Further references to this can be accessed from the official documentation and tutorial page of Pytorch website. <em>(Refer the links below).</em></p><ol type="1" id="48bb0571-20b8-4c93-97c8-a80724719a96" class="numbered-list" start="1"><li><mark class="highlight-blue"><strong><a href="https://pytorch.org/docs/stable/quantization.html">Quantization in Pytorch</a></strong></mark></li></ol><ol type="1" id="af9740a1-8426-4c41-a7dd-329eb91ae922" class="numbered-list" start="2"><li><mark class="highlight-blue"><strong><a href="https://pytorch.org/tutorials/recipes/quantization.html">Quantization Recipe</a></strong></mark></li></ol><ol type="1" id="55b0a31b-7f23-4584-b4cb-97bb41685644" class="numbered-list" start="3"><li><mark class="highlight-blue"><strong><a href="https://www.youtube.com/watch?v=0VdNflU08yA">Detailed analysis on Quantization with Pytorch</a></strong></mark></li></ol><p id="5a335270-0f25-4f77-a86d-61e408dac58c" class="">
</p><p id="d7600e80-1686-4b8d-b340-7c8fb7342353" class="">Quantization directly helps in reducing the hardware requirements, computing power and even memory. This directly improves the power consumption and the area consumed on chip or in servers. But there will be some drawback in terms of the accuracy of the model. This can be accommodated based on the degree of precession and requirements of the application system.</p><p id="d29fa123-0167-4b34-bb58-964858a2e484" class="">
</p><p id="602a86a3-e73f-4634-ab3c-8a345bb841e8" class="">This is very much employed in embedded based edge computing and neuromorphic computing which is discussed in this course. <mark class="highlight-red"><strong><a href="https://www.tinyml.org/">TinyML</a></strong></mark><mark class="highlight-red"><strong> </strong></mark>is one such opensource platform for implementing ML algorithms on microcontroller and remote battery operated processors. </p><p id="3f839703-a62a-4166-b1f4-0be5e4862024" class="">
</p><p id="e1712244-286b-4e07-a3cb-39a91eea4d5a" class="">There are methods where the model can be trained on single precession, and then can be used for validation in integer datatype. Another possibility is training and validating the model in floating point, and then inferencing in integer type. In such cases the model is retrained on the embedded or lower precession system to have better adaptation and accuracy in inferencing and validation. </p><p id="5fe19bc4-8264-4d64-b4a1-89c341414211" class="">
</p><p id="3b3b6783-23aa-4672-b7f7-5480fcf6d503" class="">These aspects can be explored further by the student, and a more detailed discussion will take place along with the lectures regarding these topics during the course.</p><p id="935fa5b2-b9e0-4085-afe9-c05fefc94412" class="">
</p><p id="a6d60d94-99d3-403b-a06a-50731729d4cc" class="">Quantization from “float32” to “float16” a half precession datatype. Here the complete model, training and validation is performed in float16. This will improve the speed of training and reduce the memory resources consumed along with the size of the model. But the accuracy will drop slightly, it is a always a trade off. </p><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="c7f78068-b00c-4f16-9015-eb0e245f499d" class="code"><code class="language-Python"># Setting the datatype to FP16 (Quantization)
# Converting the model to half precession (FP16)
# Loading the model to the particular device (CPU / GPU)
torch.set_default_dtype(torch.float16)
model = model.LeNet5_mnist(Parameter_nn.n_classes).to(device)
model_load = model.half()</code></pre><p id="415fcf70-a405-46f3-83a2-19c165f0780d" class="">
</p><p id="de9101f8-1eab-46a4-98ad-cbf171abb606" class=""><em>Add this code snippet after defining the neural network LeNet5_minist in the previous script written for the CNN implementation. Remember to add this before call the training epochs method or the validation method. This ensures the model is quantized to FP16 from default FP32.</em></p><p id="b256b50e-f147-4aaf-847a-51369dff2d2c" class="">
</p></div></details><p id="8409dd07-6276-4e64-9936-b894b1d383e4" class="">
</p><p id="cc367779-ead3-40dd-a686-1100ce8aa1cc" class="">
</p><p id="e708b904-3aa7-4ef0-96f6-c1440354a351" class="">
</p><p id="4c96d21e-0945-4717-9394-435128de40df" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>